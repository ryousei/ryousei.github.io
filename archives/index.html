<!DOCTYPE HTML>

<html lang="en">
  
<head>
  <meta charset="utf-8">
  
  <title>Archives | My Reading List</title>
  <meta name="author" content="Ryousei Takano">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="My Reading List"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <meta http-equiv="Content-Language" content="en"/>
  

  <link href="/img/favicon.png" rel="icon">
  
    <link rel="apple-touch-icon" href="/img/apple-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/img/apple-icon.png">
    

  <link rel="alternate" href="/atom.xml" title="My Reading List" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  
  <style type="text/css">
  /* Tim Pietrusky advanced checkbox hack (Android <= 4.1.2) */
body{ -webkit-animation: bugfix infinite 1s; }
@-webkit-keyframes bugfix { from {padding:0;} to {padding:0;} }

  

  
    article .post-content-index .entry{max-height: 550px; overflow:hidden;}
  
</style>

  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'null', 'auto');
  ga('send', 'pageview');
 
</script>




  
    
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400|Playball' rel='stylesheet' type='text/css'>
    
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.11.1.min.js"></script>

  



</head>


<body>
  <header id="header" class="inner"><div class="padding">
	<div class="alignleft logo">
	  <h1><a href="/">My Reading List</a></h1>
	</div>
	<nav id="main-nav" class="alignright">
		<input type="checkbox" id="toggle" />
		<label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu" onclick><i class="fa fa-bars"></i></label>
	  <ul class="menu">
	    
	      <li><a href="/">Home</a></li>
	    
	      <li><a href="/archives">Archives</a></li>
	    
	    
	  </ul>
	</nav>
	<div class="clearfix"></div>
</div>
</header>
  <div id="page-heading-wrap">
  	<div class="inner">
      <div class="padding">
    		
          <h2>A diary of a computer scientest</h2>
        
      </div>
  	</div>
  </div>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper" class="padding">
<h2 class="archive-title">Archives</h2>


  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/30/A-Comparative-Study-of-High-Performance-Computing-on-the-Cloud/">A Comparative Study of High-Performance Computing on the Cloud</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-30T02:40:49.000Z">Dec 30 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: A Comparative Study of High-Performance Computing on the Cloud<br>Authors: Aniruddha Marathe, Rachel Harris, David K. Lowenthal (University of Arizona), Bronis R. de Supinski, Barry Rountree, Martin Schulz (Lawrence Livermore National Laboratory), Xin Yuan (Florida State University)<br>2013 ACM HPDC [<a href="http://www.hpdc.org/2013/program/conference-program/" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors believe the difference between dedicated HPC clusters and Cloud platforms is smaller than perceived and the total turnaround time and cost are better determining factors than a program’s start and finish times.</li>
<li>There are a lot of performance comparisons between HPC clusters and Amazon EC2 from a provider perspective.  From a user perspective, in contrast, this work shows multiple considerations in choosing a cluster, including the expected queue wait time along with the actual cost.  When measured in total turnaround time, the cloud can outperform dedicated HPC hardware in many cases due to significant queue wait time on HPC clusters.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/30/A-Comparative-Study-of-High-Performance-Computing-on-the-Cloud/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/29/Virtual-TCP-Offload-Optimizing-Ethernet-Overlay-Performance-on-Advanced-Interconnects/">Virtual TCP Offload: Optimizing Ethernet Overlay Performance on Advanced Interconnects</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-29T05:50:10.000Z">Dec 29 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Virtual TCP Offload: Optimizing Ethernet Overlay Performance on Advanced Interconnects<br>Authors: Zheng Cui, Patrick G. Bridges (University of New Mexico), John R. Lange (University of Pittsburgh), Peter A. Dinda (Northwestern University)<br>2013 ACM HPDC [<a href="http://www.hpdc.org/2013/program/conference-program/" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present a virtual TCP offload Ethernet device, which enables the overlay system to leverage the semantics and performance characteristics of the underlying network to maximize overlay performance.</li>
<li>The authors implemented this approach by enhancing the <em>VNET/P</em> virtual network overlay [HPDC2012] with offload device capabilities (VTOEs) and running it on top of a high-performance InfiniBand network fabric.</li>
<li>To bridge the semantic gap, it uses minimal interconnect features, and translates the advanced features to a simple Ethernet interface while hiding them from the guest OS.</li>
<li>VNET/P is an in-VMM, overlay-based layer-2 virtual networking system for the <em>Palacios</em> VMM [IPDPS2010].  Their previous work [<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6468468&amp;punumber%3D6468170%26sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A6468439%29%26pageNumber%3D2" target="_blank" rel="external">SC2012</a>] shows that this model provides near-native MPI performance on a high-performance underlying Ethernet network, by using optimistic interrupts and zero-copy cut-through data forwarding (<a href="http://www.slideshare.net/oraccha/optimizing-overlaybased-virtual-networking-through-optimistic-interrupts-and-cutthrough-forwarding" target="_blank" rel="external">Related slides</a>).  However, it is unable to deliver near-native network performance on InfiniBand due to the semantic gap between the virtual overlay and underlying networks.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/29/Virtual-TCP-Offload-Optimizing-Ethernet-Overlay-Performance-on-Advanced-Interconnects/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/28/Mortar-Filling-the-Gaps-in-Data-Center-Memory/">Mortar: Filling the Gaps in Data Center Memory</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-28T00:38:54.000Z">Dec 28 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Mortar: Filling the Gaps in Data Center Memory<br>Authors: Jinho Hwang (IBM T.J. Watson Research Center), Ahsen J. Uppal, Timothy Wood, H. Howie Huang (George Washington University)<br>2014 ACM VEE [<a href="http://vee2014.cs.technion.ac.il/program.html" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>Mortar</em>, a hypervisor managed cache system for repurposing spare system memory over data center resources.</li>
<li>Data center servers are typically overprovisioned to handle unpredictable workload bursts, average 30% of memory is idle from GWU commercial data center.  And also application-level distributed cache, e.g., memcached, is heavily used in data centers.</li>
<li>A hypervisor maintains a list of spare memory that can be allocated to VMs on demand.  Mortar provides a new Linux system call and a Xen hypercall which together provide the interface to a key-value store.</li>
<li>The source code is available from <a href="https://github.com/gwcloudlab/mortar" target="_blank" rel="external">here</a>.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/28/Mortar-Filling-the-Gaps-in-Data-Center-Memory/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/27/Shrinking-the-Hypervisor-One-Subsystem-at-a-Time-a-Userspace-Packet-Switch-for-Virtual-Machines/">Shrinking the Hypervisor One Subsystem at a Time: a Userspace Packet Switch for Virtual Machines</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-27T02:01:41.000Z">Dec 27 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Shrinking the Hypervisor One Subsystem at a Time: a Userspace Packet Switch for Virtual Machines<br>Author: Julian Stecklina, TU Dresden<br>2014 ACM VEE [<a href="http://vee2014.cs.technion.ac.il/program.html" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>sv3</em>, an efficient lockless userspace network switch for VMs running on unmodified Linux/KVM.</li>
<li>The current virtualization layer (hypervisor) like QEMU/KVM is designed in a monolithic manner.  The authors apply a microkernel approach to a hypervisor design.  It contributes for minimizing Trusted Code Base (TCB).</li>
<li>Although the Xen’s driver domain model is a similar idea, sv3 uses processes to run isolated from a hypervisor without special privileges.</li>
<li>Linux/KVM has all the mechanisms to make a microkernel-style design possible and effective:<ul>
<li>rights transfer via AF_LOCAL sockets</li>
<li>efficient notifications via eventfds</li>
<li>drivers in userspace via <a href="http://lwn.net/Articles/391459/" target="_blank" rel="external">VFIO</a> using eventfds for IRQs</li>
<li>binding eventfds to VM exits and IRQ injection.</li>
</ul>
</li>
<li>The source code of sv3 including QEMU patches is available from <a href="https://github.com/blitz/sv3" target="_blank" rel="external">here</a>.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/27/Shrinking-the-Hypervisor-One-Subsystem-at-a-Time-a-Userspace-Packet-Switch-for-Virtual-Machines/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/21/XvMotion-Unified-Virtual-Machine-Migration-over-Long-Distance/">XvMotion: Unified Virtual Machine Migration over Long Distance</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-21T14:15:07.000Z">Dec 21 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: XvMotion: Unified Virtual Machine Migration over Long Distance<br>Authors: Ali José Mashtizadeh, Stanford University; Min Cai, Gabriel Tarasuk-Levin, and Ricardo Koller, VMware, Inc.; Tal Garfinkel; Sreekanth Setty, VMware, Inc.<br>2014 USENIX ATC [<a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/mashtizadeh" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>XvMotion</em>, an unified memory and storage migration over the local and wide area networks.</li>
<li>Wide are memory migration<ul>
<li>Problem: Applications can change memory faster than network bandwidth.</li>
<li>Solution: Stun During Page Send (SDPS) throttles a VM just enough to keep the dirty rate is less than the network transmit rate.</li>
</ul>
</li>
<li>Wide area storage migration<ul>
<li>Problem: Synchronous mirroring over the WAN slows guest workload.</li>
<li>Solution Asynchronous IO mirroring and throttling copy process and workload as necessary.</li>
</ul>
</li>
<li>The authors also show several IO optimization techniques including TCP extensions.</li>
<li>XvMotion is built on the live migration and IO mirroring mechanisms in the VMWare ESX.</li>
<li>The authors demonstrated that VM migration between California to India (1 Gbps network with 214 ms RTT) is practical.  The workload penalty from throttling is about 10%.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/21/XvMotion-Unified-Virtual-Machine-Migration-over-Long-Distance/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/15/SENIC-Scalable-NIC-for-End-Host-Rate-Limiting/">SENIC: Scalable NIC for End-Host Rate Limiting</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-15T12:52:18.000Z">Dec 15 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: SENIC: Scalable NIC for End-Host Rate Limiting<br>Authors: Sivasankar Radhakrishnan, University of California, San Diego; Yilong Geng and Vimalkumar Jeyakumar, Stanford University; Abdul Kabbani, Google Inc.; George Porter, University of California, San Diego; Amin Vahdat, Google Inc. and University of California, San Diego<br>2014 USENIX NSDI [<a href="https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/radhakrishnan" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>SENIC</em>, a scalable rate limiting NIC design that can support tens of thousands of rate limiters per server.</li>
<li>Network resource managers, including <em>Seawall</em> [HotCloud2010], <em>Oktopus</em> [SIGCOMM2011], and <em>EyeQ</em> [NSDI2013], require programmable rate limiters.</li>
<li>SENIC removes TX ring buffers from a NIC, because SRAM is expensive and limited.  The SENIC prototype is implemented on NetFPGA 10G, with 1000 rate limiters requiring just only 30KB SRAM.</li>
<li>The source code is available from <a href="http://jvimal.github.io/senic/" target="_blank" rel="external">here</a>.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/15/SENIC-Scalable-NIC-for-End-Host-Rate-Limiting/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/13/Software-Persistent-Memory/">Software Persistent Memory</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-13T03:04:11.000Z">Dec 13 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Software Persistent Memory<br>Authors: Jorge Guerra, Leonardo Mármol, Daniel Campello, Carlos Crespo, Raju Rangaswami, and Jinpeng Wei, Florida International University<br>2012 USENIX ATC [<a href="https://www.usenix.org/conference/atc12/technical-sessions/presentation/guerra" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>Software Persistent Memory (SoftPM)</em>, a lightweight persistent memory abstraction for C.</li>
<li>SoftPM automatically discovers persistent data dependencies from the user-defined root structure.  In the following example code, <em>cid</em> is a root structure.</li>
</ul>
<figure class="highlight C"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> l {</div><div class="line">	<span class="keyword">int</span> v;</div><div class="line">	<span class="keyword">struct</span> l *next;</div><div class="line">} <span class="built_in">list</span>;</div><div class="line"></div><div class="line"><span class="keyword">struct</span> pcont {</div><div class="line">	<span class="built_in">list</span> *head;</div><div class="line">} *C;</div><div class="line"></div><div class="line">foo() {</div><div class="line">	cid = pCAlloc(&C, <span class="keyword">sizeof</span>(*C)); <span class="comment">/* create a new container */</span></div><div class="line">	<span class="built_in">list</span> *n1 = <span class="built_in">malloc</span>(...);</div><div class="line">	C-&gt;head = n1;</div><div class="line">	n1-&gt;v = X;</div><div class="line">	<span class="built_in">list</span> *n2 = <span class="built_in">malloc</span>(...);</div><div class="line">	n2-&gt;v = Y;</div><div class="line">	n1-&gt;next = n2;</div><div class="line">	:</div><div class="line">	pPoint(cid); <span class="comment">/* create a persistent point asynchronously */</span></div><div class="line">}</div></pre></td></tr></table></figure>

<ul>
<li>There are a lot of persistent memory libraries (<em>RVM</em>, <em>SSDAlloc</em>, <em>NV-Heep</em>, <em>Mnemosyne</em>) and persistent object stores (<em>ObjectStore</em>, <em>Thor</em>). <em>Mnemosyne</em> [ASPLOS2011] and <em>NV-Heaps</em> [ASPLOS2011] also provide a persistent memory abstractions similar to SoftPM, however, they are explicitly designed for byte addressable non-volatile memories.  Some operating systems, e.g., <em>Grasshopper</em> [ComputerSystems1994], support a single level persistent store in which pointer swizzling is employed to convert persistent store references to in-memory addresses at the page granularity.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/13/Software-Persistent-Memory/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/12/Software-Techniques-for-Avoiding-Hardware-Virtualization-Exits/">Software Techniques for Avoiding Hardware Virtualization Exits</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-12T00:00:51.000Z">Dec 12 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Software Techniques for Avoiding Hardware Virtualization Exits<br>Authors: Ole Agesen, Jim Mattson, Radu Rugina, and Jeffrey Sheldon, VMware<br>2012 USENIX ATC [<a href="https://www.usenix.org/conference/atc12/technical-sessions/presentation/agesen" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present a set of software-based optimization techniques to eliminate VM exits.</li>
<li>Hardware-assisted x86 virtualization suffers from expensive VM exits, which cause guest OS/VMM transitions.  To reduce the overhead, the binary translator detects cluster/pair of instructions, which generate multiple VM exits, and translates them together to avoid VM exits.  It is also useful for a nested VM environment.</li>
<li>For example, the following code updates page table entry.  Each write causes a VM exit, and VMM updates a shadow page table entry.  Using the proposed technique, VMM inspects the next instruction at the first exit, and executes both instruction at once.</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">mov</span> <span class="number">4</span>(%<span class="number">ecx</span>), %<span class="literal">esi</span></div><div class="line"><span class="keyword">mov</span> (%<span class="number">ecx</span>), %<span class="number">ebx</span></div></pre></td></tr></table></figure>

<ul>
<li>The proposed technique has been implemented in VMWare products (Workstation, Fusion, ESX) and validated by years of use in the field.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/12/Software-Techniques-for-Avoiding-Hardware-Virtualization-Exits/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/10/Improving-Server-Application-Performance-via-Pure-TCP-ACK-Receive-Optimization/">Improving Server Application Performance via Pure TCP ACK Receive Optimization</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-09T22:37:26.000Z">Dec 10 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Improving Server Application Performance via Pure TCP ACK Receive Optimization<br>Authors: Michael Chan and David R. Cheriton, Stanford University<br>2013 USENIX ATC [<a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/chan" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>TCP-PARO</em>, (Pure ACK Receive Optimization), that removes the bulk of expensive per-packet processing and provides a fast processing path of pure ACKs in the network stack (bypasses the original stack).<ul>
<li>Pure ACK parsing and demultiplexing</li>
<li>ACK processing fast module</li>
<li>Handling a mixture of pure ACKs and other TCP segments</li>
</ul>
</li>
<li>The authors show that ACK receive processing can consume up to 20% CPU cycles in a Web server.  Using this technique, they demonstrate cycles savings of 15%, and 33% throughput improvement in reliable multicast, <em>TCP-SMO</em> [INFOCOM2002].</li>
<li>While some batching optimization have been proposed to amortize receive-side overhead for data packet, they are not suitable for small control packets like ACKs.</li>
<li>By the way, the last author is famous as his work, <em>V operating system</em>, in 80s.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/10/Improving-Server-Application-Performance-via-Pure-TCP-ACK-Receive-Optimization/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2014/12/09/Lightweight-Memory-Tracing/">Lightweight Memory Tracing</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2014-12-08T22:06:06.000Z">Dec 9 2014</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">Ryousei Takano</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>Title: Lightweight Memory Tracing<br>Authors: Mathias Payer, Enrico Kravina, and Thomas R. Gross, ETH Zurich<br>2013 USENIX ATC [<a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/payer" target="_blank" rel="external">URL</a>]</p>
<hr>
<ul>
<li>The authors present <em>memTrace</em>, a lightweight memory tracing technique that builds on dynamic on-the-fly cross-ISA binary translation of 32-bit code to 64-bit code.</li>
<li>Current software-only memory tracing incurs high performance overhead (up to 10x for <em>libdft</em> [VEE2012], 20x for <em>Valgrind</em>’s memcheck [PLDI2007], and up to 40x for <em>taintcheck</em> [NDSS2005]).  This is because every single memory access of the application is checked by additional code (memlet) and hardware is limited to a small set of watched locations (watchpoints).</li>
<li>MemTrace dynamically translates 32-bit x86 application code to x64 code (i.e., 64-bit code), and that enables memlets, code sequences that are woven into the target application code to execute memory tracing, to use extra registers and a huge shadow memory space (64-bit address space).</li>
<li>The authors demonstrates two memory tracing case studies: unlimited watchpoints and safe heap memory allocator.</li>
<li>The prototype implementation extends the <em>libdetox</em> binary translation platform [VEE2011].</li>
<li>The source code is available from <a href="https://github.com/gannimo/memTrace" target="_blank" rel="external">gannimo/memTrace@github</a>.</li>
</ul>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2014/12/09/Lightweight-Memory-Tracing/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  

  <nav id="pagination">
  
  
    <a href="/archives/page/2/" class="alignright next">Next</a>
  
  <div class="clearfix"></div>
</nav>




   </div></div>
    <aside id="sidebar" class="alignright"><div class="padding">
	
	  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>
	
	  
<div class="widget recent-post">
  <h3 class="title">Recent Posts</h3>
  <ul class="entry">
    
      <li>
        <a href="/2014/12/30/A-Comparative-Study-of-High-Performance-Computing-on-the-Cloud/">A Comparative Study of High-Performance Computing on the Cloud</a>
      </li>
    
      <li>
        <a href="/2014/12/29/Virtual-TCP-Offload-Optimizing-Ethernet-Overlay-Performance-on-Advanced-Interconnects/">Virtual TCP Offload: Optimizing Ethernet Overlay Performance on Advanced Interconnects</a>
      </li>
    
      <li>
        <a href="/2014/12/28/Mortar-Filling-the-Gaps-in-Data-Center-Memory/">Mortar: Filling the Gaps in Data Center Memory</a>
      </li>
    
      <li>
        <a href="/2014/12/27/Shrinking-the-Hypervisor-One-Subsystem-at-a-Time-a-Userspace-Packet-Switch-for-Virtual-Machines/">Shrinking the Hypervisor One Subsystem at a Time: a Userspace Packet Switch for Virtual Machines</a>
      </li>
    
      <li>
        <a href="/2014/12/21/XvMotion-Unified-Virtual-Machine-Migration-over-Long-Distance/">XvMotion: Unified Virtual Machine Migration over Long Distance</a>
      </li>
    
  </ul>
</div>

	
	  
	
	  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  
    <a href="/tags/HPC/">HPC<small>1</small></a>
  
    <a href="/tags/kernel/">kernel<small>1</small></a>
  
    <a href="/tags/list/">list<small>1</small></a>
  
    <a href="/tags/memory/">memory<small>3</small></a>
  
    <a href="/tags/networking/">networking<small>8</small></a>
  
    <a href="/tags/virtualization/">virtualization<small>12</small></a>
  
</div>

	
</div></aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="padding">
	<div class="alignleft">
	  
	  &copy; 2014 Ryousei Takano
	  
	  Powerd by <a href="http://hexo.io/" target="_blank">hexo</a>
	  and Theme by <a href="https://github.com/halfer53/metro-light" target="_blank">metro-light</a>
	</div>

	<div class="alignright">
		
		
		
		
		
		
		
	</div>

	<div class="clearfix"></div>
</div>

<div class="scroll-top"><i class="fa fa-arrow-circle-up"></i></div></footer>
  


<script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/3.0.4/jquery.imagesloaded.js"></script>
<script src="/js/gallery.js"></script>



<script type="text/javascript">
$(window).scroll(function() {

    if($(this).scrollTop() > 400) {
        $('.scroll-top').fadeIn(200);
    } else {
        $('.scroll-top').fadeOut(200);
    }
});

$('.scroll-top').bind('click', function(e) {
    e.preventDefault();
    $('body,html').animate({scrollTop:0},200);
});
</script>


</body>
</html>
